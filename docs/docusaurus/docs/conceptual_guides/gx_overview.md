---
title: Great Expectations overview
---

import CheckpointFlowchart from './images_overview/checkpoint_flowchart.png'

This guide is intended for new users of Great Expectations (GX) and those looking for a more foundational understanding of GX, its components, and its principal workflows.  You will be introduced to the moving parts of GX and how they interact without digging through the code that governs those processes and interactions.  The insight provided in this guide will help you understand the how and why of processes you work through in future tutorials.

## Great Expectations (GX)

GX is a Python library that provides a framework for describing the acceptable state of data and then validating that the data meets those criteria. 

GX enables you to build trust in your data's quality by testing how it performs against your expectations.  Reusable components and ongoing testing provide visibility into the health of your data pipeline over time. The human-readable reports generated by GX allow you to share the results of that analysis with others and better collaborate on improvements to both your data and your understanding of your data.

With GX providing insight into your data and helping assure its quality, you can ensure that your data is fully suitable to be used as intended in your processes.

## GX core components

When working with GX from end to end - that is, from installation of GX to the validation of your data - there are four core components that you will use.  These are "top level" components that interface, contain, or manage underlying objects and processes to make your work easier.  This guide will go over each of these four components and what they govern in more depth after this section. However, in brief, the four components are:

- **Data Context:** Manages the settings and metadata for a GX project, as well as providing an entry point to the GX Python API.
- **Datasources:** tell GX how to connect to your source data, and how to organize the retrieved data for future use.
- **Expectations:** tell GX the standards to which your data should conform.
- **Checkpoints:** validate a set of Expectations against a specific set of data.


## Data Context

![A Data Context](./images_overview/data_context_flowchart.png)

A Data Context defines a GX project.  In Python, the Data Context object serves as your entry point for the GX API and manages various classes to limit the objects you need to directly manage yourself.  A Data Context contains all of the metadata used by GX, the configurations for GX objects, and the output from validating data.

The following are the available Data Context types:
- **Ephemeral Data Context:** Exists in memory, and does not persist beyond the current Python session.
- **File Data Context:** Exists as a folder and configuration files. Its contents persist between Python sessions.
- **Cloud Data Context:** Supports persistence between Python sessions, but additionally serves as the entry point for Great Expectations Cloud.

- [Data Context tutorials](/docs/guides/setup/configure_data_contexts_lp)

### The GX API

A Data Context object in Python provides convenience methods to further configure and interact with GX.  These methods and the objects and additional methods accessed through them compose the GX public API.  

Through the GX public API you will be able to connect GX to your data sources, define the standards to which you expect your data to conform, retrieve data from your data sources, and validate that data against your defined standards.  When working with GX in Python, you will almost always start by instantiating an existing or new Data Context with which to access the GX public API.

- [The GX API reference](/docs/reference/api_reference)

### Settings and Stores

Stores contain the metadata GX uses.  This includes things such as configurations for GX objects, information that is recorded when GX validates data, and credentials used for accessing data sources or remote environments.  GX utilizes one Store for each type of metadata, and the Data Context contains the settings that tell GX where that Store should reside and how to access it.

A Data Context includes default configurations for your Store locations. The default location of Stores in a Filesystem or Cloud Data Context is nested within the same folder hierarchy that contains the Data Context itself. An Ephemeral Data Context will default to keeping your Stores in memory. You can update these defaults to specify other storage locations, such as hosting them in a shared cloud environment or external folders.

- [Stores tutorials](/docs/guides/setup/setup_overview_lp)

### Data Docs

Data Docs are human-readable documentation generated by GX.  In general, Data Docs describe the standards that you expect your data to conform to, and the results of validating your data against those standards.  The storage and retrieval of this information is managed by the Data Context.

Data Docs exist as webpages.  By default, a new Data Context includes a configuration for building them locally for File Data Contexts, in memory for Ephemeral Data Contexts, and online for Cloud Data Contexts.

You can configure where your Data Docs are hosted.  Unlike Stores, you can define configurations for multiple Data Docs sites.  You can also specify what information each Data Doc site provides, allowing you to format and provide different Data Docs for different use cases.

- [Data Docs tutorial](/docs/guides/setup/configuring_data_docs/host_and_share_data_docs)

## Datasources

![Datasource overview](./images_overview/datasource_flowchart.png)

Datasources connect GX to your source data.  Your source data could be CSV files in a folder, a PostgreSQL database hosted on AWS, or any combination of numerous data formats and environments.  Regardless of your source data's format and where it resides, Datasources provide GX with a unified API for working with it.

- [Datasource tutorials](/docs/guides/connecting_to_your_data/connect_to_data_lp)

### Data Assets and Batches

Data Assets are collections of records within a Datasource.  While a Datasource tells GX how to connect to your source data, Data Assets tell GX how to organize that data.  For instance, you could define a Data Asset for a SQL Datasource as all the records within a specific table.  For a File Datasource, you could define a Data Asset as all of the `.csv` files in a specific subfolder.

Data Assets allow you to organize your data into collections of records that do not necessarily conform to the schema of your source data.  For instance, in a SQL Datasource you can use a Query Data Asset to combine an arbitrary set of records into a single Data Asset.  Likewise, in a File Datasource you can use regular expressions (regex) to group a specific set of files into a single Data Asset based on their filenames.

Data Assets can be further partitioned into Batches.  Batches are unique subsets of records within a Data Asset.  For example, say you have a Data Asset in a SQL Datasource that consisted of all records from last year in a given table.  You could then partition those records into Batches of data that correspond to the records for individual months of that year.

Using Batches to further partition a Data Asset is optional.  However, having a list for Batches in your Data Asset is not.  This is because GX uses Batches to specify the data to work with.  By default, if you do not further partition the data in a Data Asset it will contain a list with only one Batch.  In that case, the single Batch will consist of all the records in that Data Asset.

- [Data Asset and Batch tutorials](/docs/guides/connecting_to_your_data/manage_data_assets_lp)

### Batch Requests

A Batch Request specifies one or more Batches within the Data Asset.  Batch Requests are the primary way of retrieving data for use in GX.  Because Batch Requests can specify multiple Batches they provide significant flexibility in how you utilize the data in a single Data Asset.

As an example, GX can automate the process of running statistical analyzes for multiple Batches of data.  This is possible because you can provide a Batch Request that corresponds to multiple Batches in a Data Asset.  Alternatively, you can specify a single Batch from that same Data Asset so that you do not need to re-run the analyzes on all of your data when you are only interested in a single subset.

Taking the example of a Data Asset that has been partitioned into Batches by months, this would let you build a statistical model off of each month of your existing data by providing all of the Batches in a single Batch Request.  Then, after an update, you could specify that you only wanted to run your analysis on the most recent month's data by providing a Batch Request that only indicates that one Batch.

- [Batch Request tutorial](/docs/guides/connecting_to_your_data/fluent/batch_requests/how_to_request_data_from_a_data_asset)

## Expectations

An Expectation is a verifiable assertion about Source Data.  Similar to assertions in traditional Python unit tests, Expectations provide a flexible, declarative language for describing expected behaviors. Unlike traditional unit tests which describe the expected behavior of code given a specific input, Expectations apply to the input data itself. For example, you can define an Expectation that a column contains no null values. When GX runs that Expectation on your data it generates a report which indicates if a null value was found.

Expectations improve quality for data applications by giving you a verifiable standard to measure your data against.  They also enhance communication about your data by giving you a source of truth that describes the state it should conform to. Expectations help you take the implicit assumptions about your data and make them explicit and shareable, reducing the need to consult subject matter experts about uncertainties and let you avoid leaving insights about data in isolated silos.

Expectations can be built directly from the domain knowledge of subject matter experts, interactively while introspecting a set of data, or through automated tools provided by GX.

For a list of available Expectations, see [the Expectation Gallery](https://greatexpectations.io/expectations/).


- [Expectation Tutorials](/docs/guides/expectations/expectations_lp)

### Expectation Suites

Expectation Suites are collections of Expectations describing your data.  When GX validates data, an Expectation Suite helps streamline the process by running all of the contained Expectations against that data.  In almost all cases, when you create an Expectation you will be creating it inside of an Expectation Suite object.

You can define multiple Expectation Suites for the same data to cover different use cases.  An example could be having one Expectation Suite for raw data, and a more strict Expectation Suite for that same data post-processing.  Because an Expectation Suite is decoupled from a specific source of data, you can apply the same Expectation Suite against multiple, disparate Datasources.  For instance, you can reuse an Expectation Suite that was created around an older set of data to validate the quality of a new set of data.

- [Expectations and Expectation Suite tutorials](/docs/guides/expectations/create_manage_expectations_lp)

### Data Assistants

A Data Assistant is a utility that automates the process of building Expectations by asking questions about your data, gathering information to describe what is observed, and then presenting Metrics and proposed Expectations based on the answers it has determined.  This can greatly accelerate the process of creating Expectations for the provided data.

- [Data Assistant tutorials](/docs/guides/expectations/profilers_data_assistants_lp)

### Custom Expectations

GX comes with a built-in library of more than 50 common expectations.  However, you can also create custom Expectations if you need something more specialized for your data validation.  You can even use custom Expectations that have been contributed by other GX community members by installing them as Plugins.

- [Tutorials for creating custom Expectations](/docs/guides/expectations/custom_expectations_lp)
- [Tutorials for using custom Expectations](/docs/guides/expectations/creating_custom_expectations/how_to_use_custom_expectations)

## Checkpoints

![Checkpoint overview](./images_overview/checkpoint_flowchart.png)

A Checkpoint is the primary means for validating data in a production deployment of GX. Checkpoints provide an abstraction for bundling a Batch (or Batches) of data with an Expectation Suite (or several), and then running those Expectations against the paired data.

- [Checkpoint tutorials](/docs/guides/validation/checkpoints/checkpoint_lp)

### Validation Results

The Validation Results returned by GX tell you how your data corresponds to what you expected of it. You can view this information in the Data Docs that are configured in your Data Context. Evaluating your Validation Results helps you identify issues with your data. If the Validation Results show that your data meets your Expectations, you can confidently use it.

- [Validation Result further reading](/docs/terms/validation_result)

### Actions

One of the most powerful features of Checkpoints is that you can configure them to run Actions. The Validation Results generated when a Checkpoint runs determine what Actions are performed. Typical use cases include sending email, Slack, or custom notifications. Another common use case is updating Data Docs sites. Actions can be used to do anything you are capable of programming in Python. Actions are a versatile tool for integrating Checkpoints in your pipeline's workflow.

- [Actions tutorials](docs/guides/validation/validation_actions/actions_lp)