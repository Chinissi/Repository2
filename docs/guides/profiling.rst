.. _profiling:

================================================================================
Profiling
================================================================================

Profiling evaluates a data asset to create candidate expectations that describe the data. Profiling helps build
custom expectation suites and :ref:`data_documentation` that reflects expected and observed characteristics of data.
Together, Profiling, Documentation, and Validation are the three core services offered by GE.

Profiling a batch of data from a data asset produces an expectation_suite and a validation_result.

The expectation_suite includes all expectations generated by that profiler. The :class:`~great_expectations.profile.\
basic_dataset_profiler.BasicDatasetProfiler` included in GE will generate a large number of very loosely-specified
expectations. Effectively it is asserting that the given statistic is relevant for evaluating batches of that data
asset, but it is not yet sure what the statistic's value should be. For example, when it encounters a numeric column,
``BasicDatasetProfiler`` will add an ``expect_column_mean_to_be_between`` expectation but choose the min_value and
max_value to both be None: essentially saying only that it expects the mean to exist.

The validation_result contains the output of that expectation suite when validated against the same batch of data. So,
in the scenario described above of a numeric column for which the ``BasicDatasetProfiler`` created a loosely-specified
expectation about the column mean, the validation_result returned will include the actual observed_value for the mean
of that column.

Running a profiler on a data asset may produce a large number of expectations which should be reviewed
closely before transferring them to a new expectation suite used for validation in a pipeline. Notably,
expectation suites produced by profilers can include expectations that would fail even on the batch of data from
which they were generated. However, having a comprehensive list of loosely-specified expectations can be extremely
valuable for generating :ref:`data_documentation`.


Profiling is still a beta feature in Great Expectations. Over time, we plan to extend and improve the ``BasicDatasetProfiler`` and also add additional profilers.

Warning: ``BasicDatasetProfiler`` will evaluate the entire batch
without limits or sampling, which may be very time consuming. As a rule of thumb, we recommend starting with batches
smaller than 100MB.


How to Run Profiling
--------------------

Run During Init
~~~~~~~~~~~~~~~~~~~~~~

The ``great_expectations init`` command offers to profile a newly added datasource. If you agree, data assets in that
datasource will be profiled (e.g., tables in the database). By default the profiler will select the first 20 data
assets.

Expectation suites generated by the profiler will be saved in the configured ``expectations`` directory for expectation
suites. The expectation suite name by default is the name of hte profiler that generated it. Validation results will be
saved in the ``uncommitted/validations`` directory by default; the CLI will then offer to move them to the
``fixtures/validations`` directory from which data documentation is built.


Run From Command Line
~~~~~~~~~~~~~~~~~~~~~~

The GE command-line interface can also profile a datasource:

.. code-block:: bash

    great_expectations profile DATASOURCE_NAME

Just as when running during init, expectation suites generated by the profiler will be saved in the configured
``expectations`` directory for expectation suites. The expectation suite name by default is the name of hte profiler
that generated it. Validation results will be saved in the ``uncommitted/validations`` directory by default. When
running profile directly on a datasource, the CLI will not offer to move resulting validations to the
``fixtures/validations`` directory from which data documentation is built. If you want to generate HTML documentations
from these results, you have to move the files into 'fixtures/validations':

.. image:: ../images/movie_db_profiling_screenshot_3.jpg
    :height: 400px

and then run this command to generate HTML:

.. code-block:: bash

    great_expectations documentation

See :ref:`data_documentation` for more information.

Run From Jupyter Notebook
~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want to profile just one data asset in a datasource (e.g., one table in the database), you can do it using
Python in a Jupyter notebook:

.. code-block:: python

    from great_expectations.profile.basic_dataset_profiler import BasicDatasetProfiler

    # obtain the DataContext object
    context = ge.data_context.DataContext()

    # load a batch from the data asset
    batch = context.get_batch('ratings')

    # run the profiler on the batch - this returns an expectation suite and validation results for this suite
    expectation_suite, validation_result = BasicDatasetProfiler.profile(batch)

    # save the resulting expectation suite with a custom name
    context.save_expectation_suite(expectation_suite, "ratings", "my_profiled_expectations")


How Are Expectations And Profiling Related?
-------------------------------------------

In order to characterize a data asset, profiling creates an expectation suite. Unlike the expectations that are
typically used for data validation, these expectations do not necessarily apply any constraints. This is an example of
``expect_column_mean_to_be_between`` expectations that supplies null as values for both min and max. This means that
profiling does not expect the mean to be within a particular range--anything is acceptable.

.. code-block:: json

    {
      "expectation_type": "expect_column_mean_to_be_between",
      "kwargs": {
        "column": "rating",
        "min_value": null,
        "max_value": null
      }
    }

When this expectation is evaluated against a batch, the validation result computes the actual mean and returns it as
observed_value. Getting this observed value was the sole purpose of the expectation.

.. code-block:: json

    {
      "success": true,
      "result": {
        "observed_value": 4.05,
        "element_count": 10000,
        "missing_count": 0,
        "missing_percent": 0
      }
    }

Custom Profilers
----------------------

Like most things in Great Expectations, Profilers are designed to be extensibile. You can develop your own profiler buy subclassing ``BasicDataSet`` profiler, or from the parent ``DataAssetProfiler`` class it self. For help, advice, and ideason developing custom profilers, please get in touch on `the Great Expectations slack channel <https://tinyurl.com/great-expectations-slack>`_.



Known Issues
------------

When profiling CSV files, the profiler makes assumptions, such as considering the first line to be the header.
Overriding these assumptions is currently possible only when running profiling in Python by passing extra arguments to
get_batch.

