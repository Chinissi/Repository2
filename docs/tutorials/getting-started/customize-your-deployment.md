---
title: Optional - Customize your deployment
---

At this point, you have your first, working local deployment of Great Expectations. You’ve also been introduced to the foundational concepts in the library: Data Contexts, Datasources, Expectations, Profilers, Data Docs, Validation, and Checkpoints.

Congratulations! You’re off to a very good start.

The next step is to customize your deployment by upgrading specific components of your deployment. Data Contexts make this modular, so that you can add or swap out one component at a time. Most of these changes are quick, incremental steps—so you can upgrade from a basic demo deployment to a full production deployment at your own pace and be confident that your Data Context will continue to work at every step along the way.

This last section of this tutorial is designed to present you with clear options for upgrading your deployment. For specific implementation steps, please check out the linked How-to guides.

Components
Here’s an overview of the components of a typical Great Expectations deployment:

* Great Expectations configs and metadata
  * Options for storing Great Expectations configuration
  * Options for storing Expectations
  * Options for storing Validation Results
  * Options for customizing generated notebooks

* Integrations to related systems
  * Additional Datasources and Generators
  * Options for hosting Data Docs
  * Additional Validation Operators and Actions
  * Options for triggering Validation

## Options for storing Great Expectations configuration
The simplest way to manage your Great Expectations configuration is usually by committing great_expectations/great_expectations.yml to git. However, it’s not usually a good idea to commit credentials to source control. In some situations, you might need to deploy without access to source control (or maybe even a file system).

Here’s how to handle each of those cases:

* How to use a YAML file or environment variables to populate credentials
* How to populate credentials from a secrets store
* How to instantiate a Data Context without a yml file

## Options for storing Expectations
Many teams find it convenient to store Expectations in git. Essentially, this approach treats Expectations like test fixtures: they live adjacent to code and are stored within version control. git acts as a collaboration tool and source of record.

Alternatively, you can treat Expectations like configs, and store them in a blob store. Finally, you can store them in a database.

* How to configure an Expectation store in Amazon S3
* How to configure an Expectation store in GCS
* How to configure an Expectation store in Azure blob storage
* How to configure an Expectation store to PostgreSQL

## Options for storing Validation Results
By default, Validation Results are stored locally, in an uncommitted directory. This is great for individual work, but not good for collaboration. The most common pattern is to use a cloud-based blob store such as S3, GCS, or Azure blob store. You can also store Validation Results in a database.

* How to configure a Validation Result store on a filesystem
* How to configure a Validation Result store in S3
* How to configure a Validation Result store in GCS
* How to configure a Validation Result store in Azure blob storage
* How to configure a Validation Result store to PostgreSQL

## Options for customizing generated notebooks
Great Expectations generates and provides notebooks as interactive development environments for expectation suites. You might want to customize parts of the notebooks to add company-specific documentation, or change the code sections to suit your use-cases.

* How to configure notebooks generated by “suite edit”

## Additional Datasources and Generators
Great Expectations plugs into a wide variety of Datasources, and the list is constantly getting longer. If you have an idea for a Datasource not listed here, please speak up in the public discussion forum.

* How to configure a Pandas/filesystem Datasource
* How to configure a Pandas/S3 Datasource
* How to configure a Redshift Datasource
* How to configure a Snowflake Datasource
* How to configure a BigQuery Datasource
* How to configure a Databricks Azure Datasource
* How to configure an EMR Spark Datasource
* How to configure a Databricks AWS Datasource
* How to configure a self managed Spark Datasource

## Options for hosting Data Docs
By default, Data Docs are stored locally, in an uncommitted directory. This is great for individual work, but not good for collaboration. A better pattern is usually to deploy to a cloud-based blob store (S3, GCS, or Azure blob store), configured to share a static website.

* How to host and share Data Docs on a filesystem
* How to host and share Data Docs on S3
* How to host and share Data Docs on Azure Blob Storage
* How to host and share Data Docs on GCS

## Additional Validation Operators and Actions
Most teams will want to configure various Validation Actions as part of their deployment.

* How to update Data Docs as a Validation Action
* How to store Validation Results as a Validation Action
* How to trigger Slack notifications as a Validation Action

If you also want to modify your :ref:reference__core_concepts__validation__validation_operator, you can learn how here:

* How to add a Validation Operator

## Options for triggering Validation

There are two primary patterns for deploying Checkpoints. Sometimes Checkpoints are executed during data processing (e.g. as a task within Airflow). From this vantage point, they can control program flow. Sometimes Checkpoints are executed against materialized data. Great Expectations supports both patterns. There are also some rare instances where you may want to validate data without using a Checkpoint.

* How to run a Checkpoint in Airflow
* How to run a Checkpoint in python
* How to run a Checkpoint in terminal
* How to validate data without a Checkpoint


