{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import great_expectations as ge\n",
    "import great_expectations.jupyter_ux\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Expectations For Your CSV Files\n",
    "\n",
    "When you develop your data pipeline code, you make some assumptions about what valid input data looks like.\n",
    "You can encode these assumptions as *expectations* (e.g., \"column X should not have more than 5% null values\").\n",
    "\n",
    "Once you deploy your code in production, Great Expectations will validate new data and check if it conforms to the assumptions your code makes.\n",
    "\n",
    "This way you can stop data that your code does not know how to deal with from being processed, thus avoiding the \"garbage in, garbage out\" problem.\n",
    "\n",
    "In this notebook you will create expectations for the CSV files your pipeline processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataContext object\n",
    "\n",
    "First, we need to create a `DataContext` object - it represents Great Expectations in your data pipeline.\n",
    "We are passing '../../' to this object to let it know where to find its configuration. No need to modify this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = ge.data_context.DataContext('../../', expectation_explorer=True)\n",
    "context = ge.data_context.DataContext('../../', expectation_explorer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "\n",
    "\n",
    "Data sources are locations where your pipeline reads its input data from. In our case, it is a directory on the local file system.\n",
    "\n",
    "When you ran `great_expectations init` in your project, you configured a data source of type \"pandas\" and gave it a name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = great_expectations.jupyter_ux.set_data_source(context, data_source_type='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_source_name = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Great Expectations we use the name \"data asset\" for each \"type\" of files.\n",
    "\n",
    "Let's say that your data pipeline processes CSV files in `/data/my_input_directory` directory on the filesystem.\n",
    "CSV files that contain orders lines are deposited in the subdirectory `orders` and the ones contain cancellations lines in `cancellations`. Each CSV file has date and/or sequence number in its name.\n",
    "\n",
    "Following this example, this directory will looks like this:\n",
    "\n",
    "    my_input_directory\n",
    "        ├── orders\n",
    "        |   └── orders_20190101_1.csv        \n",
    "        |   └── orders_20190102_1.csv        \n",
    "        |   └── orders_20190103_1.csv        \n",
    "        ├── cancellations\n",
    "        |   └── cancellations_20190101_1.csv        \n",
    "        |   └── cancellations_20190102_1.csv        \n",
    "        |   └── cancellations_20190103_1.csv        \n",
    "\n",
    "In this example there are 2 data assets: \"orders\" and \"cancellations\". You can create expectations about these types.\n",
    "\n",
    "In order to create expectations about a data asset (e.g., orders), you will need to load one of the files of this type\n",
    "into Great Expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_expectations.jupyter_ux.list_available_data_asset_names(context, data_source_name=data_source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pick one of the data asset names above and use as the value of data_asset_name argument below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = context.get_batch(data_source_name, data_asset_name=\"orders\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you need to pass options to read_csv (e.g., sep, header, etc), you can add them as arguments in the method call below. Once you have all your options, add them to the config of this datasource in great_expectations.yml under \"read_csv_kwargs\" key**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call in the cell above loaded one of the batches of this data asset. \n",
    "When working with files, batch corresponds to one file\n",
    "You can read more on this here:\n",
    "https://great-expectations.readthedocs.io/en/latest/what_are_batches.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you can see which file was loaded\n",
    "df._batch_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Expectations\n",
    "\n",
    "Now that you have one of the files loaded, you can call expect* methods on the dataframe in order to check\n",
    "if you can make an assumption about the data.\n",
    "\n",
    "For example, to check if you can expect values in column \"order_date\" to never be empty, call: `df.expect_column_values_to_not_be_null('order_date')`\n",
    "\n",
    "### How do I know which types of expectations I can add?\n",
    "* *Tab-complete* this statement, and add an expectation of your own; copy the cell to add more\n",
    "* In jupyter, you can also use *shift-tab* to see the docstring for each expectation, to see what parameters it takes and get more information about the expectation.\n",
    "* Here is a glossary of expectations you can add:\n",
    "https://great-expectations.readthedocs.io/en/latest/glossary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example:\n",
    "\n",
    "column_name = df.columns[0]\n",
    "df.expect_column_values_to_not_be_null(column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more expectations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more expectations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more expectations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review the expectations.\n",
    "\n",
    "Expectations that were true on this data sample were added. To view all the expectations you added so far about this type of files, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.get_expectations_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the expectations about this type of files. Expectations for \"orders\" in our example will be saved in a JSON file in great_expectations/data_asset_configurations directory. We will load this file when we need to validate.\n",
    "\n",
    "\n",
    "      your_project_root\n",
    "        ├── great_expectations\n",
    "        |   └── expectations\n",
    "        |     └── orders.json        \n",
    "        |     └── cancellations.json        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.save_expectations_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You created and saved expectations for at least one of the types of CSV files your data pipeline processes. \n",
    "\n",
    "### We will show you how to set up validation - the process of checking if new files of this type conform to your expectations before they are processed by your pipeline's code. \n",
    "\n",
    "### Go to [integrate_validation_into_pipeline.ipynb](integrate_validation_into_pipeline.ipynb) to proceed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
