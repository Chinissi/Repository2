{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import great_expectations as ge\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Expectations For Your CSV Files\n",
    "\n",
    "As your data products and models are developed, you can encode assumptions about input and output datasets as **expectations**.\n",
    "\n",
    "Using that workflow provides the following benefits:\n",
    "\n",
    "1. These are machine verifiable and can be used to monitor data flowing through your pipelines.\n",
    "2. These eliminate poisonous implicit assumptions that cause data engineers re-work and waste time - \"How do we define visits?\"\n",
    "3. These **will eventually** be easy to edit.\n",
    "4. These **will eventually** be easy to reason about visually.\n",
    "\n",
    "\n",
    "Let's say that your data pipeline processes CSV files in `/data/my_input_directory` directory on the filesystem.\n",
    "CSV files that contain orders lines are deposited in the subdirectory `orders` and the ones contain cancellations lines - \n",
    "in `cancellations`. Each CSV file has date and/or sequence number in its name.\n",
    "\n",
    "Following this example, this directory will looks like this:\n",
    "\n",
    "    my_input_directory\n",
    "        ├── orders\n",
    "        |   └── orders_20190101_1.csv        \n",
    "        |   └── orders_20190102_1.csv        \n",
    "        |   └── orders_20190103_1.csv        \n",
    "        ├── cancellations\n",
    "        |   └── cancellations_20190101_1.csv        \n",
    "        |   └── cancellations_20190102_1.csv        \n",
    "        |   └── cancellations_20190103_1.csv        \n",
    "\n",
    "Your code that processes these files as they arrive makes some assumptions on what a valid file looks like.\n",
    "You can encode these assumptions as expectations (e.g., \"column X should not have more than 5% null values\").\n",
    "\n",
    "When you validate new files to check if they conform to the assumptions your code makes, you can stop data that your code\n",
    "does not know how to deal with from being processed, thus avoiding the \"garbage in, garbage out\" problem.\n",
    "\n",
    "First, you have to author your expectations for every type of file your code processes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataContext object\n",
    "\n",
    "First, we need to create a `DataContext` object - it represents Great Expectations in your data pipeline.\n",
    "We are passing '../../' to this object to let it know where to find its configuration. No need to modify this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = ge.data_context.DataContext('../../', expectation_explorer=True)\n",
    "context = ge.data_context.DataContext('../../', expectation_explorer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "\n",
    "\n",
    "data sources are locations where your pipeline reads its input data from. In our case, it is a directory - \n",
    "\n",
    "When you ran `great_expectations init` in your project, you configured a data source of type \"filesystem\" and gave it a name (\"my_input_directory\" in our example).\n",
    "\n",
    "In the following cell set data_source_name to your data source name.\n",
    "\n",
    "If you did not create the data source during init, here is how to add it now: \n",
    "https://great-expectations.readthedocs.io/en/latest/how_to_add_data_source.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = \"my_input_directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Great Expectations we use the name \"data asset\" for each \"type\" of files (e.g., orders and cancellations).\n",
    "\n",
    "In order to create expectations about a data asset (e.g., orders), you will need to load one of the files of this type\n",
    "into Great Expectations. `df` below will behave like a regular Pandas dataframe, but with additional methods added by Great Expectations - you will see shortly.\n",
    "\n",
    "In the next cell we are calling context.get_data_asset to load one of the files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = context.get_data_asset(data_source_name, data_asset_name=\"orders\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Expectations\n",
    "\n",
    "Now that you have one of the files loaded, you can call expect* methods on the dataframe in order to check\n",
    "if you can make an assumption about the data.\n",
    "\n",
    "For example, to check if you can expect values in column \"order_date\" to never be empty, call: `df.expect_column_values_to_not_be_null('order_date')`\n",
    "\n",
    "\n",
    "Here is a glossary of expectations you can add:\n",
    "https://great-expectations.readthedocs.io/en/latest/glossary.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review the expectations.\n",
    "\n",
    "Expectations that were true on this data sample were added. To view all the expectations you added so far about this type of files, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.get_expectations_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the expectations about this type of files. Expectations for \"orders\" in our example will be saved in a JSON file in great_expectations/data_asset_configurations directory. We will load this file when we need to validate.\n",
    "\n",
    "\n",
    "      your_project_root\n",
    "        ├── great_expectations\n",
    "        |   └── expectations\n",
    "        |     └── orders.json        \n",
    "        |     └── cancellations.json        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.save_expectations_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you created and saved expectations for at least one of the types of CSV files your data pipeline processes, we will show you how to set up validation - the process of checking if new files of this type conform to your expectations before they are processed by your pipeline's code. \n",
    "\n",
    "Check out the notebook \"validate_csv_files.ipynb\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
