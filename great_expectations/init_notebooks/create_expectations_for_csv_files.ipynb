{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import great_expectations as ge\n",
    "import great_expectations.jupyter_ux\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Expectations For Your CSV Files\n",
    "\n",
    "When you develop your data pipeline or analytics code, you make some assumptions about what valid input data looks like.\n",
    "You can encode these assumptions as *expectations* (e.g., \"column X should not have more than 5% null values\").\n",
    "\n",
    "Once you deploy your code in production, Great Expectations will validate new data and check if it conforms to the assumptions your code makes.\n",
    "\n",
    "This way you can stop data that your code does not know how to deal with from being processed, thus avoiding the \"garbage in, garbage out\" problem.\n",
    "\n",
    "First, you have to author your expectations for every type of file your code processes.\n",
    "\n",
    "In this notebook you can create expectations for CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataContext object\n",
    "\n",
    "First, we need to create a `DataContext` object - it represents Great Expectations in your data pipeline.\n",
    "We are passing '../../' to this object to let it know where to find its configuration. No need to modify this line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = ge.data_context.DataContext('../../', expectation_explorer=True)\n",
    "context = ge.data_context.DataContext('../../', expectation_explorer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasource\n",
    "\n",
    "\n",
    "Datasources are locations where your pipeline reads its input data. In our case, it is a directory on the local file system.\n",
    "\n",
    "When you ran `great_expectations init` in your project, you configured a datasource of type \"pandas\" and gave it a name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = great_expectations.jupyter_ux.set_data_source(context, 'pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_source_name = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Great Expectations we use the name \"data asset\" for each \"type\" of file.\n",
    "\n",
    "Let's say that your data pipeline processes CSV files in `/data/my_input_directory` directory on the filesystem.\n",
    "\n",
    "CSV files that contain orders lines are deposited in the subdirectory `orders` and the ones contain cancellations lines in `cancellations`. Each CSV file has date and/or sequence number in its name.\n",
    "\n",
    "Following this example, this directory will look like this:\n",
    "\n",
    "    my_input_directory\n",
    "        ├── orders\n",
    "        │   ├── orders_20190101_1.csv        \n",
    "        │   ├── orders_20190102_1.csv        \n",
    "        │   └── orders_20190103_1.csv        \n",
    "        ├── cancellations\n",
    "        │   ├── cancellations_20190101_1.csv        \n",
    "        │   ├── cancellations_20190102_1.csv        \n",
    "        │   └── cancellations_20190103_1.csv        \n",
    "\n",
    "In this example there are 2 data assets: \"orders\" and \"cancellations\". You can create expectations about these types.\n",
    "\n",
    "In order to create expectations about a data asset (e.g., orders), you will need to load one of the files of this type into Great Expectations.\n",
    "\n",
    "Great Expectations returns structured json objects, but the jupyter_ux module helps structure and add context to raw return objects when working interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_expectations.jupyter_ux.list_available_data_asset_names(context, data_source_name=data_source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick one of the data asset names above to use as the value of data_asset_name argument below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = context.get_data_asset(data_source_name, data_asset_name=\"orders\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Expectations\n",
    "\n",
    "Now that you have one of the files loaded, you can call expect* methods on the dataframe in order to check\n",
    "if you can make an assumption about the data.\n",
    "\n",
    "For example, to check if you can expect values in column \"order_date\" to never be empty, call: `df.expect_column_values_to_not_be_null('order_date')`\n",
    "\n",
    "### How do I know which types of expectations I can add?\n",
    "* Type df.expect and hit the Tab key to enable code autocomplete.\n",
    "* Here is a glossary of expectations you can add:\n",
    "https://great-expectations.readthedocs.io/en/latest/glossary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example:\n",
    "\n",
    "column_name = df.columns[0]\n",
    "df.expect_column_values_to_not_be_null(column_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue to explore your dataset and add expectations\n",
    "\n",
    "Great expectations will track all expectations that you evaluate on a dataset, and also remember which ones returned success on their last run. Using Great Expectations while exploring a dataset can therefore help track your data exploration so you can retain insights gathered during exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tab-complete this statement, and add an expectation of your own; copy the cell to add more\n",
    "# In jupyter, you can also use shift-tab to see the docstring for each expectation, to see what parameters it takes and get more information about the expectation.
    "\n",
    "df.expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review the expectations.\n",
    "\n",
    "To view all expectations you added so far about this type of data asset, run the following cell. You can also add parameters to include even expectations that failed on their last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.get_expectations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the expectations about this type of data asset. Expectations for \"orders\" in our example will be saved in a JSON file in the great_expectations/expectations directory. We will load this file when we need to validate.\n",
    "\n",
    "\n",
    "      your_project_root\n",
    "        ├── great_expectations\n",
    "        |   └── expectations\n",
    "        |     ├── orders.json        \n",
    "        |     └── cancellations.json        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.save_expectations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you created and saved expectations for at least one of the types of CSV files your data pipeline processes, we will show you how to set up validation - the process of checking if new files of this type conform to your expectations before they are processed by your pipeline's code. \n",
    "\n",
    "Check out the notebook \"validate_csv_files.ipynb\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
