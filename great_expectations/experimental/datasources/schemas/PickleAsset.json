{
    "title": "PickleAsset",
    "description": "Load pickled pandas object (or any object) from file.\n\n.. warning::\n\n   Loading pickled data received from untrusted sources can be\n   unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    File path, URL, or buffer where the pickled object will be loaded from.\n\n    .. versionchanged:: 1.0.0\n       Accept URL. URL is not limited to S3 and GCS.\n\ncompression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n    If 'infer' and 'path_or_url' is path-like, then detect compression from\n    the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n    compression) If 'infer' and 'path_or_url' is not path-like, then use\n    None (= no decompression).\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib`` as header options. For other URLs (e.g.\n    starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n    ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n\n    .. versionadded:: 1.2.0\n\nReturns\n-------\nunpickled : same type as object stored in file\n\nSee Also\n--------\nDataFrame.to_pickle : Pickle (serialize) DataFrame object to file.\nSeries.to_pickle : Pickle (serialize) Series object to file.\nread_hdf : Read HDF5 file into a DataFrame.\nread_sql : Read SQL query or database table into a DataFrame.\nread_parquet : Load a parquet object, returning a DataFrame.\n\nNotes\n-----\nread_pickle is only guaranteed to be backwards compatible to pandas 0.20.3.\n\nExamples\n--------\n>>> original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})\n>>> original_df\n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n>>> pd.to_pickle(original_df, \"./dummy.pkl\")\n\n>>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")\n>>> unpickled_df\n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n\n>>> import os\n>>> os.remove(\"./dummy.pkl\")",
    "type": "object",
    "properties": {
        "name": {
            "title": "Name",
            "type": "string"
        },
        "type": {
            "title": "Type",
            "default": "pickle",
            "enum": [
                "pickle"
            ],
            "type": "string"
        },
        "order_by": {
            "title": "Order By",
            "type": "array",
            "items": {
                "$ref": "#/definitions/BatchSorter"
            }
        },
        "base_directory": {
            "title": "Base Directory",
            "type": "string",
            "format": "path"
        },
        "regex": {
            "title": "Regex",
            "type": "string",
            "format": "regex"
        },
        "compression": {
            "title": "Compression",
            "default": "infer",
            "anyOf": [
                {
                    "type": "string"
                },
                {
                    "type": "object"
                }
            ]
        },
        "storage_options": {
            "title": "Storage Options",
            "type": "object"
        }
    },
    "required": [
        "name",
        "base_directory",
        "regex"
    ],
    "additionalProperties": false,
    "definitions": {
        "BatchSorter": {
            "title": "BatchSorter",
            "type": "object",
            "properties": {
                "key": {
                    "title": "Key",
                    "type": "string"
                },
                "reverse": {
                    "title": "Reverse",
                    "default": false,
                    "type": "boolean"
                }
            },
            "required": [
                "key"
            ]
        }
    }
}
